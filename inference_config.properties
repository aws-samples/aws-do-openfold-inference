openfold_model_name=finetuning_ptm_2
processor=gpu 
output_dir_base=/fsx-shared/openfold/cameo/inference_output/
alignment_dir=/fsx-shared/openfold/cameo/r6i_xlarge_run3_16GB/
config_preset=model_1_ptm
use_precomputed_alignments=True
skip_relaxation=True
no_cpus=4
# number of models per model server
num_models=1

# service_port=8080 - port on which model service will be exposed
service_port=8080

runtime=kubernetes
# Kubernetes-specific deployment settings
# instance_type = c5.xxx | g4dn.xlarge | g4dn.12xlarge | inf1.xlarge | inf1.6xlarge | ...
# A node group with the specified instance_type must exist in the cluster
# The instance type must have the processor type configured above
instance_type=g4dn.xlarge
# num_servers - number of model servers to deploy
# note that more than one model server can run on a node with multiple cpu/gpu/inferentia chips.
# example: 4 model servers fit on one inf1.6xlarge instance as it has 4 inferentia chips.
num_servers=1
# Kubernetes namespace
##namespace=mpi
# Kubernetes app name
app_name=openfold-${processor}
app_dir=app-${app_name}-${instance_type}
